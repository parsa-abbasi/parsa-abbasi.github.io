[{"authors":null,"categories":null,"content":"Parsa Abbasi is a graduate student in Artificial Intelligence at Iran University of Science and Technology, where he is conducting research focused on improving graph neural networks for relational graphs. His research seeks to improve upon existing architectures like R-GAT by proposing and developing a model with fewer training parameters that addresses static attention issues.\nParsa received his BSc in Computer Engineering from the University of Guilan, where he explored areas such as natural language processing and deep learning. During his undergraduate studies, Parsa co-authored a research paper on designing sentiment analysis models using deep learning architectures for low-resource Persian language.\nDriven by his passion for the field, Parsa seeks to pursue a PhD in AI to continue his research and further advance the field.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"15a3404cbbc999059255baca3346a34d","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Parsa Abbasi is a graduate student in Artificial Intelligence at Iran University of Science and Technology, where he is conducting research focused on improving graph neural networks for relational graphs. His research seeks to improve upon existing architectures like R-GAT by proposing and developing a model with fewer training parameters that addresses static attention issues.","tags":null,"title":"Parsa Abbasi","type":"authors"},{"authors":[],"categories":null,"content":"Data Preparation Data preparation is a critical step in the data analysis process, involving the transformation of raw data into a format that can be analyzed by data scientists or machine learning algorithms. The primary steps involved in data preparation include:\nData Cleaning Data Integration Data Transformation Data Reduction Data Discretization However, it’s important to note that the specific steps and order in which they are performed may differ depending on the data set and analysis goals.\nData preparation is a big part of the whole Data Science process. According to CrowdFlower report in 2016, it is seen that out of 80 data scientist, they will spend their day in the following: Let’s take the example of music streaming platform, SoundCloud, and explore how each of these steps could be applied to their data.\nData Cleaning SoundCloud has access to vast amounts of data from its users, but this data can contain duplicate, corrupt, inaccurate, or irrelevant information. The platform must handle such data to ensure accuracy and avoid analytical errors. Some challenges that SoundCloud might face during data cleaning are:\nIncomplete or inconsistent user information: SoundCloud may have difficulty analyzing its users’ music preferences if there is missing or inaccurate account information, such as age, gender, or location. For example, if SoundCloud wants to analyze the most popular songs among its young users in the United States, incomplete or inconsistent user information could lead to inaccurate results.\nSongs with incomplete metadata: : Sound metadata includes information like artist name, album, release date, etc. Any inconsistencies or inaccuracies in metadata can affect various analysis tasks, like analyzing top albums or artists. For example, if a song is mislabeled with an incorrect artist name, it could lead to inaccurate analysis of an artist’s popularity.\nInconsistent Artist Names: There may be different variations of artists’ names on SoundCloud, like incorrectly spelled names, abbreviated names, or stage names that can lead to inconsistent track listings. Inconsistent artist names can lead to misrepresentation of an artist’s most popular tracks or top plays. For example, using different spellings of an artist name like “Nick Cave” as “Nic Cave” or “Nicholas Edward Cave” will make the total number of plays for each track under her name dispersed inaccurately.\nData Integration SoundCloud may have data from multiple sources such as user profiles, song playlists, album data, and song metadata. All these data sources can be integrated to create a comprehensive dataset. This data integration can help in exploring correlations between songs, user preferences, and identifying popular artists, genres, and playlists.\nData Transformation The data needs to be transformed into a usable format for analysis. For example, SoundCloud may group users by location, age, or behavior to create new variables or features. They may also perform feature engineering to create new variables. For example, SoundCloud might transform user data into a score indicating how active and engaged each user is on the platform, based on metrics such as likes, comments, and track reposts.\nThis step might also include Normalization, which scales data to a specific range to eliminate the effects of the scale of the feature on the analysis.\nData Reduction Organizations may choose to focus on a subset of the data for analysis, such as popular songs or active users. For example, SoundCloud may only consider songs with more than 1,000 plays, or users who have listened to more than 10 songs in the past week.\nData Discretization Continuous data may be discretized to create categories or groups for analysis. For example, Soundcloud may group users by age ranges (e.g. 18-24, 25-34, etc.) or create a new variable to indicate the popularity of a song (e.g. unpopular, popular, very popular).\nNumPy and Pandas So now that we have a better understanding of the data preparation process, let’s learn how to process data efficiently using the numpy and pandas libraries. Now let’s learn everything we need to know about these two libraries! Slide 1: Introduction to NumPy Exercise 1: Analyzing Job Offers (Initial | Solution) Slide 2: Introduction to Pandas Data Visualization Data visualization is the process of creating graphical representations of data to communicate information effectively. It is an essential tool for data analysts as it enables them to explore and analyze large datasets quickly and easily, providing insights that may not be immediately apparent from a table of numbers.\nThe need for data visualization arises because humans are visual creatures and we process information more efficiently when it is presented in a visual format. A well-designed graph or chart can tell a story, highlight trends and patterns, and reveal relationships in the data that might be missed when analyzing raw data.\nMatplotlib, Seaborn, and Plotly Now that we …","date":1680775200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680775200,"objectID":"752c6ffc36e5a71d8302febe1de2939a","permalink":"https://parsa-abbasi.github.io/talk/data-preparation-and-visualization-in-python/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/talk/data-preparation-and-visualization-in-python/","section":"event","summary":"This class focuses on the techniques and tools required for data preparation and data visualization in python.","tags":["Data Analysis","NumPy","Pandas","Matplotlib","Seaborn","Plotly","Data Wrangling","Data Visualization"],"title":"Data Preparation and Visualization in Python","type":"event"},{"authors":["Parsa Abbasi"],"categories":[],"content":"Introduction to NumPy Installation | Documentation\nLast updated: March 28, 2023 What is NumPy? NumPy, short for Numerical Python, is a fundamental library for data analysis and scientific computing in the Python programming language.\nWhy NumPy? (1) Multidimensional Arrays: NumPy arrays can have any number of dimensions, which makes it possible to store and manipulate complex data sets.\nHigh Performance: NumPy core is based on a highly optimized C implementation, which means that it can perform mathematical and numerical operations much faster than pure Python.\nWhy NumPy? (2) Mathematical Functions: NumPy provides a wide variety of mathematical functions for operations, including statistics, linear algebra, and Fourier Transforms.\nEfficient and Fast Computation: NumPy allows for fast and accurate computation through powerful vectorized operations and optimized mathematical functions.\nN-dimensional array One of the key features of NumPy is its N-dimensional array object, or ndarray, which is a fast, flexible container for large datasets in Python.\nNote: An ’ndarray’ is a multidimensional, homogeneous array which means that all the elements in the array are of the same type.\nCreating a NumPy array (1) The easiest way to create an ndarray is to use the array() function and pass any sequence-like object (e.g. a list, tuple, or another array) to it.\nimport numpy as np data = [1, 2.5, 3.1, 4, 5.6] arr = np.array(data) array([1. , 2.5, 3.1, 4. , 5.6]) Creating a NumPy array (2) import numpy as np data = [[1, 2, 3, 4], [5, 6, 7, 8]] arr = np.array(data) array([[1, 2, 3, 4], [5, 6, 7, 8]]) Number of dimensions We can check the number of dimensions of an array using the ndim attribute.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.ndim 2 Shape of an array The shape attribute returns a tuple of integers indicating the size of the array in each dimension.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.shape (2, 4) Size of an array The size attribute returns the total number of elements of the array.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.size 8 Visualization Reshaping an array We can change the shape of an array by using:\narr.reshape(new_shape) where new_shape is a tuple of integers indicating the new shape of the array.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.reshape((4, 2)) array([[1, 2], [3, 4], [5, 6], [7, 8]]) Flattening an array We can flatten an array by using the flatten method.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.flatten() array([1, 2, 3, 4, 5, 6, 7, 8]) We can also use the ravel or reshape(-1) methods to flatten an array.\nData type (1) The dtype attribute is an object describing the type of the elements in the array. Unless specified, NumPy tries to infer a good data type for the array that it creates.\narray([[1, 2, 3, 4], [5, 6, 7, 8]]) arr.dtype dtype(\u0026#39;int64\u0026#39;) Data type (2) A full list of NumPy data types can be found here.\nType Type code Description bool ? Boolean (True or False) stored as a byte int8 i1 Byte (-128 to 127) int16 i2 Integer (-32768 to 32767) int32 i4 Integer (-2147483648 to 2147483647) int64 i8 Integer (-9223372036854775808 to 9223372036854775807) uint8 u1 Unsigned integer (0 to 255) float16 f2 Half precision float: sign bit, 5 bits exponent, 10 bits mantissa float32 f4 Single precision float: sign bit, 8 bits exponent, 23 bits mantissa float64 f8 Double precision float: sign bit, 11 bits exponent, 52 bits mantissa string_ S Fixed-length ASCII string type (1 byte per character) unicode_ U Fixed-length Unicode type (number of bytes platform specific) object O Python object type Data type examples (1) Boolean\narray = np.array([True, False, True], dtype=bool) array([ True, False, True]) String\narray = np.array([\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;numpy\u0026#39;], dtype=np.string_) array([b\u0026#39;hello\u0026#39;, b\u0026#39;world\u0026#39;, b\u0026#39;numpy\u0026#39;], dtype=\u0026#39;|S5\u0026#39;) Data type examples (2) Unicode\narray = np.array([u\u0026#39;سلام\u0026#39;,u\u0026#39;بله\u0026#39;,u\u0026#39;خیر\u0026#39;], dtype=np.unicode_) array([\u0026#39;سلام\u0026#39;, \u0026#39;بله\u0026#39;, \u0026#39;خیر\u0026#39;], dtype=\u0026#39;\u0026lt;U4\u0026#39;) Object\narray = np.array([{\u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;age\u0026#34;: 25}, [1, 2, 3], \u0026#34;hello\u0026#34;], dtype=object) array([{\u0026#39;name\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;age\u0026#39;: 25}, list([1, 2, 3]), \u0026#39;hello\u0026#39;], dtype=object) Casting data type You can explicitly cast an array from one dtype to another using ndarray’s astype method.\narray = np.array([1, 2, 3, 4, 5]) array([1, 2, 3, 4, 5]) array.astype(np.float64) array([1., 2., 3., 4., 5.]) Note: Calling astype always creates a copy of the data, even if the new dtype is the same as the old dtype. Array creation functions (1) zeros and ones create arrays of 0’s or 1’s, respectively, with a given length or shape. empty creates an array without initializing its values to any particular value.\nnp.zeros(10) array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) np.ones((2, 6)) array([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]]) np.empty((1, 2)) array([[1., 1.]]) Array creation functions (2) zeros_like and ones_like create arrays of 0’s or 1’s with the same shape and dtype as a given array.\narray = np.array([[1, 2, 3], [4, 5, 6]]) array([[1, 2, 3], [4, 5, 6]]) np.zeros_like(array) …","date":1679961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679961600,"objectID":"da492a9346c707ed1d01ed9dc53fc9f8","permalink":"https://parsa-abbasi.github.io/slides/numpy/","publishdate":"2023-03-28T00:00:00Z","relpermalink":"/slides/numpy/","section":"slides","summary":"An introduction to NumPy library for data analysis and scientific computing in Python.","tags":["NumPy"],"title":"Introduction to NumPy","type":"slides"},{"authors":null,"categories":null,"content":"Signature verification has practical applications in various fields, particularly for security, legal, and financial purposes. Deep learning (DL) models have been proved to be promising in recognizing handwritten signatures, with high accuracy. In this expriment, we investigate the use of the InceptionV3, a pre-trained convolutional neural network (CNN), for signature classification.\nThis project was done as part of the Pattern Recognition course at the Iran University of Science and Technology in the Fall of 2020. Dataset The dataset used for this project is called UTSig, which consists of 115 classes, each belonging to one authentic person. There are 27 genuine signatures per class, 3 opposite-hand signed samples, and 42 simple forgeries. The dataset can be downloaded from here.\nA sample of the UTSig dataset. To start the implementation, the dataset is downloaded and stored in the Google Drive. The genuine signatures were divided into 22 training images and 5 test images for each person and placed in the appropriate subdirectorie.\n!unzip \u0026#39;/content/drive/MyDrive/UTSig.zip\u0026#39; dataset_path = \u0026#39;/content/UTSig_Crop/\u0026#39; # Number of genuine signatures per class (person) G_num = 27 # Number of forgeries signatures per class (person) F_num = 45 # Number of classes (persons) c_num = 115 os.mkdir(dataset_path + \u0026#39;genuine\u0026#39;) os.mkdir(dataset_path + \u0026#39;genuine/train\u0026#39;) os.mkdir(dataset_path + \u0026#39;genuine/test\u0026#39;) dataset_gen_path = dataset_path + \u0026#39;genuine/\u0026#39; Organizing the Dataset We organized the images into the appropriate format for the ImageDataGenerator. We created subdirectories for each class in the genuine directory, with 22 images for training and 5 images for testing per class.\ngenuine train class_0 0_image_0.png 0_image_1.png … 0_image_21.png class_1 … class_114 test class_0 0_image_22.png 0_image_23.png … 0_image_26.png class_1 … class_114 for c in trange(c_num): class_id = c + 1 class_id_str = str(class_id).zfill(len(str(c_num))) # Make a subdirectory for this class train_subdirectory = dataset_gen_path + \u0026#39;/train/class_\u0026#39; + str(c) os.mkdir(train_subdirectory) test_subdirectory = dataset_gen_path + \u0026#39;/test/class_\u0026#39; + str(c) os.mkdir(test_subdirectory) # For each genuine for g in range(G_num): genuine_id = g + 1 genuine_id_str = str(genuine_id).zfill(len(str(G_num))) file_name = \u0026#39;C\u0026#39; + class_id_str + \u0026#39;G\u0026#39; + genuine_id_str + \u0026#39;.PNG\u0026#39; file_path = dataset_path + file_name if g \u0026lt; 22: dest_path = train_subdirectory + \u0026#39;/\u0026#39; + str(c) + \u0026#39;_image_\u0026#39; + str(g) + \u0026#39;.png\u0026#39; os.replace(file_path, dest_path) else: dest_path = test_subdirectory + \u0026#39;/\u0026#39; + str(c) + \u0026#39;_image_\u0026#39; + str(g) + \u0026#39;.png\u0026#39; os.replace(file_path, dest_path) Preprocessing We used the Image Data Generator from Keras to preprocess our images. We also applied various augmentation techniques such as rotation, zooming, shifting, and flipping to generate more data and reduce overfitting.\nsrc_path_train = dataset_gen_path + \u0026#39;/train/\u0026#39; src_path_test = dataset_gen_path + \u0026#39;/test/\u0026#39; train_datagen = ImageDataGenerator( rescale = 1 / 255.0, rotation_range= 20, zoom_range = 0.05, width_shift_range = 0.05, height_shift_range = 0.05, shear_range = 0.05, horizontal_flip = True, fill_mode = \u0026#34;nearest\u0026#34;, validation_split = 0.20) test_datagen = ImageDataGenerator(rescale = 1 / 255.0) batch_size = 16 target_size = (299, 299) train_generator = train_datagen.flow_from_directory( directory = src_path_train, target_size = target_size, color_mode = \u0026#34;rgb\u0026#34;, batch_size = batch_size, class_mode = \u0026#34;categorical\u0026#34;, subset = \u0026#39;training\u0026#39;, shuffle = True ) valid_generator = train_datagen.flow_from_directory( directory = src_path_train, target_size = target_size, color_mode = \u0026#34;rgb\u0026#34;, batch_size = batch_size, class_mode = \u0026#34;categorical\u0026#34;, subset = \u0026#39;validation\u0026#39;, shuffle = True ) test_generator = test_datagen.flow_from_directory( directory = src_path_test, target_size = target_size, color_mode = \u0026#34;rgb\u0026#34;, batch_size = 1, class_mode = None, shuffle = False ) test_true_labels = test_generator.classes Model The propsed model based on InceptionV3 To classify the signatures, we used the InceptionV3 model, which is pre-trained on the ImageNet database to classify images into 1000 object categories. We modified the model by removing the fully-connected layer at the top, setting all layers to be non-trainable, and adding a 2D Global Average Pooling to transform the feature embedding into a single 2048 size vector. We then added a fully connected layer with 1024 neurons, a Dropout layer, and a softmax layer to calculate the score for each of the classes.\n# Loading Inception-V3 model model = InceptionV3(include_top=False, weights=\u0026#39;imagenet\u0026#39;, input_shape=(299, 299, 3)) # Freeze layers for layer in model.layers: layer.trainable = False # Add our classifier to the end of the model flat1 = GlobalAveragePooling2D()(model.layers[-1].output) class1 = Dense(1024, activation=\u0026#39;relu\u0026#39;)(flat1) dropout1 = Dropout(0.1)(class1) output = Dense(c_num, activation=\u0026#39;softmax\u0026#39;)(dropout1) model = Model(inputs=model.inputs, outputs=output) model.summary() Training and …","date":1677096900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677096900,"objectID":"91caf2294239b88c2b777d4b9e271b68","permalink":"https://parsa-abbasi.github.io/project/signature-verification-with-cnn/","publishdate":"2023-02-22T20:15:00Z","relpermalink":"/project/signature-verification-with-cnn/","section":"project","summary":"This project explores the implementation of the InceptionV3 model for signature classification in the UTSig dataset.","tags":["Deep Learning","Convolutional Neural Networks","CNN","InceptionV3","Keras"],"title":"Offline Signature Verification with Convolutional Neural Networks (CNNs)","type":"project"},{"authors":["Javad PourMostafa Roshan Sharami","Parsa Abbasi Sarabestani","Seyed Abolghasem Mirroshandel"],"categories":null,"content":"","date":1586563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586563200,"objectID":"e5ffdae38699b8502e5850f7574e52dc","permalink":"https://parsa-abbasi.github.io/publication/deepsentipers/","publishdate":"2020-04-11T00:00:00Z","relpermalink":"/publication/deepsentipers/","section":"publication","summary":"This paper focuses on how to extract opinions over each Persian sentence-level text. Deep learning models provided a new way to boost the quality of the output. However, these architectures need to feed on big annotated data as well as an accurate design. To best of our knowledge, we do not merely suffer from lack of well-annotated Persian sentiment corpus, but also a novel model to classify the Persian opinions in terms of both multiple and binary classification. So in this work, first we propose two novel deep learning architectures comprises of bidirectional LSTM and CNN. They are a part of a deep hierarchy designed precisely and also able to classify sentences in both cases. Second, we suggested three data augmentation techniques for the low-resources Persian sentiment corpus. Our comprehensive experiments on three baselines and two different neural word embedding methods show that our data augmentation methods and intended models successfully address the aims of the research.","tags":["Text Mining","Sentiment Analysis","Deep Learning","Machine Learning","Natural Language Processing","Opinion Mining"],"title":"DeepSentiPers: Novel Deep Learning Models Trained Over Proposed Augmented Persian Sentiment Corpus","type":"publication"},{"authors":["Javad PourMostafa Roshan Sharami","Parsa Abbasi Sarabestani","Seyed Abolghasem Mirroshandel"],"categories":null,"content":"","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"0a9c99fd11fae90b52056b0641cc4624","permalink":"https://parsa-abbasi.github.io/publication/sentiment-fa/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/publication/sentiment-fa/","section":"publication","summary":"In this article, due to the importance of sentiment analysis, it has been strived to design a framework that is capable to distinguish the polarity of the opinions in the Persian language. For reaching out the goals of the article, a Persian corpus is used and three baseline models such as Support vector machine, Naive Bayes and Stochastic gradient descend have been selected in order to classify the sentence-level of a dataset. Then common deep learning models that are suitable for text mining like long short-term memory and convolutional neural network applied on embedded vectors. Through this paper, some methods based on data augmentation, word embedding, and word representation have been used also. Consequently, it will be shown that the proposed deep learning models work with high accuracy in comparison with the baseline algorithms.","tags":["Text Mining","Sentiment Analysis","Deep Learning","Machine Learning","Natural Language Processing","Opinion Mining"],"title":"Presenting A Sentiment Analysis System Using Deep Learning Models On Persian Texts (In Persian)","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://parsa-abbasi.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]